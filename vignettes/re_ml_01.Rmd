---
title: "re_ml_01"
author: "Vera Buenzli"
date: "2025-05-24"
output: 
    html_document:
      toc: true
---
# Introduction
This report evaluates the performance of **linear regression** and **K-Nearest Neighbors (KNN)** models using flux data. The analysis compares model performance in terms of **bias-variance trade-off** and investigates the **optimal choice of k** for KNN. Additionally, we visualize temporal variations in **Gross Primary Productivity (GPP)** as predicted by both models.

```{r}
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(caret)
library(recipes)

"Report Exercise - Supervised machine learning I"
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(caret)
library(recipes)

# Load data from GitHub
url_daily_fluxes <- "https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv"
daily_fluxes <- read_csv(url_daily_fluxes)

# select only the variables we are interested in
daily_fluxes <- daily_fluxes %>% 
  dplyr::select(TIMESTAMP,
              GPP_NT_VUT_REF,    # the target
              ends_with("_QC"),  # quality control info
              ends_with("_F"),   # includes all all meteorological covariates
              -contains("JSB")   # weird useless variable
) |>
  
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all meteorological covariates
                -contains("JSB")) %>%
  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) %>%
  # set all -9999 to NA
  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) %>% 
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(
    GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
    TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
    SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
    LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
    VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
    PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
    P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
    WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)
  ) %>% 
  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))


lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes)

caret::train(
  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
  data = daily_fluxes |> drop_na(),  # drop missing values
  trControl = caret::trainControl(method = "none"),  # no resampling
  method = "lm"
)

caret::train(
  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
  data = daily_fluxes |> drop_na(), 
  trControl = caret::trainControl(method = "none"),
  method = "knn"
)

set.seed(123)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

plot_data <- daily_fluxes_train |> 
  dplyr::mutate(split = "train") |> 
  dplyr::bind_rows(daily_fluxes_test |> 
                     dplyr::mutate(split = "test")) |> 
  tidyr::pivot_longer(cols = 2:9, names_to = "variable", values_to = "value")

plot_data |> 
  ggplot(aes(x = value, y = ..density.., color = split)) +
  geom_density() +
  facet_wrap(~variable, scales = "free")

pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train) |> 
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

caret::train(
  pp, 
  data = daily_fluxes_train, 
  method = "knn",
  trControl = caret::trainControl(method = "none")
)

daily_fluxes |> 
  summarise(across(where(is.numeric), ~quantile(.x, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE))) |> 
  t() |> 
  as_tibble(rownames = "variable") |> 
  setNames(c("variable", "min", "q25", "q50", "q75", "max"))

pp_prep <- recipes::prep(pp, training = daily_fluxes_train)

daily_fluxes_juiced <- recipes::juice(pp_prep)

daily_fluxes_baked <- recipes::bake(pp_prep, new_data = daily_fluxes_train)

# confirm that juice and bake return identical objects when given the same data
all_equal(daily_fluxes_juiced, daily_fluxes_baked)

# prepare data for plotting
plot_data_original <- daily_fluxes_train |> 
  dplyr::select(one_of(c("SW_IN_F", "VPD_F", "TA_F"))) |> 
  tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = "var", values_to = "val")

plot_data_juiced <- daily_fluxes_juiced |> 
  dplyr::select(one_of(c("SW_IN_F", "VPD_F", "TA_F"))) |> 
  tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = "var", values_to = "val")

# plot density
plot_1 <- ggplot(data = plot_data_original, aes(val, ..density..)) +
  geom_density() +
  facet_wrap(~var)

# plot density by var
plot_2 <- ggplot(data = plot_data_juiced, aes(val, ..density..)) +
  geom_density() +
  facet_wrap(~var)

# combine both plots
cowplot::plot_grid(plot_1, plot_2, nrow = 2)

visdat::vis_miss(
  daily_fluxes,
  cluster = FALSE, 
  warn_large_data = FALSE
)

pp |> 
  step_impute_median(all_predictors())

pp |> 
  step_impute_knn(all_predictors(), neighbors = 5)

# original data frame
df <- tibble(id = 1:4, color = c("red", "red", "green", "blue"))
df

# after one-hot encoding
dmy <- dummyVars("~ .", data = df, sep = "_")
data.frame(predict(dmy, newdata = df))

recipe(GPP_NT_VUT_REF ~ ., data = daily_fluxes) |> 
  step_dummy(all_nominal(), one_hot = TRUE)

caret::nearZeroVar(daily_fluxes, saveMetrics = TRUE)

pp |> 
  step_zv(all_predictors())

plot_1 <- ggplot(data = daily_fluxes, aes(x = WS_F, y = ..density..)) +
  geom_histogram() +
  labs(title = "Original")

plot_2 <- ggplot(data = daily_fluxes, aes(x = log(WS_F), y = ..density..)) +
  geom_histogram() +
  labs(title = "Log-transformed")

cowplot::plot_grid(plot_1, plot_2)

recipes::recipe(WS_F ~ ., data = daily_fluxes) |>   # it's of course non-sense to model wind speed like this
  recipes::step_log(all_outcomes())

pp <- recipes::recipe(WS_F ~ ., data = daily_fluxes_train) |>
  recipes::step_BoxCox(all_outcomes())

prep_pp <- recipes::prep(pp, training = daily_fluxes_train |> drop_na())
daily_fluxes_baked <- bake(prep_pp, new_data = daily_fluxes_test |> drop_na())
daily_fluxes_baked |>
  ggplot(aes(x = WS_F, y = ..density..)) +
  geom_histogram() +
  labs(title = "Box-Cox-transformed")

recipes::recipe(WS_F ~ ., data = daily_fluxes) |>
  recipes::step_YeoJohnson(all_outcomes())

# Data cleaning: looks ok, no obviously bad data
# no long tail, therefore no further target engineering
daily_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()

# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(recipes::all_predictors()) |> 
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)

# make model evaluation into a function to reuse code
eval_model <- function(mod, df_train, df_test){
  
  # add predictions to the data frames
  df_train <- df_train |> 
    drop_na()
  df_train$fitted <- predict(mod, newdata = df_train)
  
  df_test <- df_test |> 
    drop_na()
  df_test$fitted <- predict(mod, newdata = df_test)
  
  # get metrics tables
  metrics_train <- df_train |> 
    yardstick::metrics(GPP_NT_VUT_REF, fitted)
  
  metrics_test <- df_test |> 
    yardstick::metrics(GPP_NT_VUT_REF, fitted)
  
  # extract values from metrics tables
  rmse_train <- metrics_train |> 
    filter(.metric == "rmse") |> 
    pull(.estimate)
  rsq_train <- metrics_train |> 
    filter(.metric == "rsq") |> 
    pull(.estimate)
  
  rmse_test <- metrics_test |> 
    filter(.metric == "rmse") |> 
    pull(.estimate)
  rsq_test <- metrics_test |> 
    filter(.metric == "rsq") |> 
    pull(.estimate)
  
  # visualise as a scatterplot
  # adding information of metrics as sub-titles
  plot_1 <- ggplot(data = df_train, aes(GPP_NT_VUT_REF, fitted)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
    labs(subtitle = bquote( italic(R)^2 == .(format(rsq_train, digits = 2)) ~~
                              RMSE == .(format(rmse_train, digits = 3))),
         title = "Training set") +
    theme_classic()
  
  plot_2 <- ggplot(data = df_test, aes(GPP_NT_VUT_REF, fitted)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
    labs(subtitle = bquote( italic(R)^2 == .(format(rsq_test, digits = 2)) ~~
                              RMSE == .(format(rmse_test, digits = 3))),
         title = "Test set") +
    theme_classic()
  
  out <- cowplot::plot_grid(plot_1, plot_2)
  
  return(out)
}

# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)

# KNN
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

# Interpretation
The plot shows a U-shaped curve. Low k values overfit (low training error, high test error), while high k values underfit (high training and test error).
The optimal k minimizes the MAE on the test set, balancing bias and variance

# Conclusion
This report highlights the trade-offs between linear regression and KNN:
Linear regression is simple but may underfit complex data.
KNN can capture complexity but is sensitive to the choice of k, requiring careful tuning to avoid over- or underfitting.

# bias-variance trade-off

Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?
KNN is a highly flexible model that can closely fit the training data, potentially overfitting it. This results in low bias and high variance. In contrast, linear regression is simpler with a stronger assumption of linearity, leading to higher bias but lower variance. Therefore, KNN's test performance degrades more sharply compared to its training performance.

Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?
Despite its variance, KNN can capture complex patterns that linear regression misses. If the true relationship between predictors and the target is nonlinear, KNN has an advantage, leading to better performance on unseen data.

How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?
Linear Regression: High bias, low variance.
KNN: Low bias, high variance.
The best model depends on the data complexity and noise.

# The Role of k in KNN

# Hypothesis
Based on the theory of K-Nearest Neighbors (KNN), we hypothesize the following behavior for the Mean Absolute Error (MAE) on the training and test datasets as the number of neighbors \( k \) varies:
When \( k \) approaches 1, the model fits very closely to the training data, resulting in low bias but high variance. This means the training MAE will be very low, but the test MAE is expected to be high due to overfitting.
When \( k \) approaches the total number of observations \( n \), the model effectively averages over all points, resulting in high bias but low variance. In this case, both training and test MAE will be high because the model underfits the data.
Therefore, the optimal \( k \) lies between these extremes, balancing bias and variance to minimize the test error.

This behavior reflects the classic bias-variance trade-off: small \( k \) values lead to complex models that fit the training data too well (overfitting), while large \( k \) values produce overly simple models that fail to capture underlying patterns (underfitting).

# Function to test hypothesis with different k values
```{r}
library(caret)
library(dplyr)
library(ggplot2)

train_data <- daily_fluxes_train
test_data <- daily_fluxes_test

k_values <- c(1, 3, 5, 7, 10, 20, 30, 40, 50)  # keep k small

test_k <- function(k_value) {
  cat("Training with k =", k_value, "\n")
  model <- train(
    GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F,
    data = drop_na(train_data),
    method = "knn",
    tuneGrid = data.frame(k = k_value),
    trControl = trainControl(method = "none")
  )
  preds <- predict(model, newdata = drop_na(test_data))
  mae <- mean(abs(preds - drop_na(test_data)$GPP_NT_VUT_REF))
  return(mae)
}

mae_values <- sapply(k_values, test_k)

plot_df <- data.frame(k = k_values, MAE = mae_values)

ggplot(plot_df, aes(x = k, y = MAE)) +
  geom_line() + geom_point() +
  scale_x_log10() +
  labs(title = "KNN Test MAE vs. Number of Neighbors (k)",
       x = "k (log scale)",
       y = "Test MAE")
```

# Explanation of overfitting and underfitting regions & finding optimal k
For very small \( k \) (close to 1), the test MAE is high due to **overfitting**, where the model captures noise in the training data and fails to generalize.
For very large \( k \) (close to \( n \)), the model **underfits**, leading to high bias and again poor performance on test data.
The "sweet spot" or optimal \( k \) is where the test MAE reaches its minimum, indicating the best balance between bias and variance, and thus best model generalizability.